# Code Interpreter with AWS ECS

**LLM based python execution pipeline** built on **AWS**.  
User prompts are sent through an API endpoint, processed serverlessly, and executed inside a scalable ECS Fargate container.

---

## Overview

This project demonstrates how to:

- Accept user prompts via **API Gateway**
- Orchestrate execution with **AWS Lambda**
- Query and reason over **Amazon Redshift** tables using an agent deployed on **Amazon Bedrock** and generate a python code.
- Run the python code generated by the agent on **ECS Fargate**.
- Any outputs produced by the agent are saved in **s3** (see `/results`)

The agent understands the database schema and can:
- Answer analytical questions about the data
- Generate SQL queries
- Build and execute machine learning models from the tables

The github repo is connected to the **CodePipeline** that will automatically create docker image and store it in **ECR**.

---

## Data Sources

There are two tables stored in **Amazon Redshift** (see `/input`):

- **`users`**
- **`interactions`**

The LLM agent is aware of these schemas and can reason over them directly.

---

## Architecture
```mermaid
flowchart TD
    GitHub[ğŸ“ GitHub Repository] -->|Code Push| CodePipeline[ğŸ”„ AWS CodePipeline<br/>CI/CD Automation]
    CodePipeline -->|Build & Push| ECR[ğŸ“¦ Amazon ECR<br/>Docker Registry]
    
    Client([ğŸ‘¤ Client]) -->|POST /prompt| APIGW[ğŸŒ API Gateway<br/>HTTP API]
    APIGW --> Lambda[âš¡ AWS Lambda<br/>Request Handler]
    Lambda --> ECS[ğŸ“¦ ECS Fargate Task  <br/>Execution Environment]
    ECR -->|Pull Docker Image| ECS
    ECS --> LLM[ğŸ¤– Agent / Program Execution <br/>Code Interpreter / (IAM role)<br/>]
    
    LLM -->|Fetch credentials| SM[ğŸ” AWS Secrets Manager<br/>DB Credentials]
    SM -.->|Return credentials| LLM
    LLM -->|Query data| Redshift[ğŸ—ƒï¸ Amazon Redshift<br/>users & interactions tables]
    Redshift -.->|Return data| LLM
    
    LLM -->|Store outputs| S3[ğŸ—„ï¸ Amazon S3<br/>Output Storage]
    
    style GitHub fill:#f0f0f0
    style CodePipeline fill:#e1f0ff
    style ECR fill:#e1f0ff
    style Client fill:#e1f5ff
    style APIGW fill:#fff4e1
    style Lambda fill:#ffe1f5
    style ECS fill:#e1ffe1
    style LLM fill:#f5e1ff
    style SM fill:#ffe8e1
    style Redshift fill:#e8e1ff
    style S3 fill:#ffe1e1
```
---

## Components

### API Gateway
- **Type:** HTTP API
- **Endpoint:** `POST /prompt`
- **Format:** JSON

#### Example Request
```
api_gateway_url = "https://<enter-API-here>/default/mbda_python_executor"

prompt = '''
    Make a machine learning model from 'interaction' table. Use 'purchase' column for the target variable.
    '''

payload = {"prompt": prompt}
headers = {"Content-Type": "application/json"}
response = requests.post(api_gateway_url, json=payload, headers=headers)
```

---
